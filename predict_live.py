import cv2
import mediapipe as mp
import joblib
import time
from collections import deque
import speech_recognition as sr
import pyttsx3
import threading
import numpy as np
import os
import statistics  # Sá»¬ Dá»¤NG THÆ¯ VIá»†N NÃ€Y Äá»‚ THAY THáº¾ SCIPY.STATS

# --- Biáº¿n cá» vÃ  biáº¿n tráº¡ng thÃ¡i toÃ n cá»¥c ---
speech_thread_active = False
tts_thread_active = False
text_display = ""
current_mode = None


# --- CÃ¡c hÃ m chá»©c nÄƒng cho STT vÃ  TTS ---
def nghe_va_chuyen_thanh_van_ban():
    """Chá»©c nÄƒng nháº­n dáº¡ng giá»ng nÃ³i tá»« micro."""
    global speech_thread_active
    recognizer = sr.Recognizer()
    mic = sr.Microphone()
    recognized_text_output = None
    with mic as source:
        print("DEBUG_STT: Äang Ä‘iá»u chá»‰nh tiáº¿ng á»“n mÃ´i trÆ°á»ng...")
        try:
            recognizer.adjust_for_ambient_noise(source, duration=2)
        except Exception as e:
            print(f"DEBUG_STT: Lá»—i khi Ä‘iá»u chá»‰nh tiáº¿ng á»“n: {e}")
            speech_thread_active = False
            return None
        print("DEBUG_STT: ðŸŽ¤ Vui lÃ²ng nÃ³i...")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)
        except sr.WaitTimeoutError:
            print("DEBUG_STT: KhÃ´ng cÃ³ Ã¢m thanh nÃ o Ä‘Æ°á»£c phÃ¡t hiá»‡n.")
            speech_thread_active = False
            return None
        except Exception as e:
            print(f"DEBUG_STT: Lá»—i khi nghe tá»« micro: {e}")
            speech_thread_active = False
            return None
    try:
        print("DEBUG_STT: ðŸ”„ Äang xá»­ lÃ½ giá»ng nÃ³i...")
        recognized_text_output = recognizer.recognize_google(audio, language="vi-VN")
        print(f"DEBUG_STT: âœ… Báº¡n Ä‘Ã£ nÃ³i: {recognized_text_output}")
    except sr.UnknownValueError:
        print("DEBUG_STT: âš ï¸ KhÃ´ng thá»ƒ nháº­n dáº¡ng Ä‘Æ°á»£c giá»ng nÃ³i.")
    except sr.RequestError as e:
        print(f"DEBUG_STT: Lá»—i káº¿t ná»‘i tá»›i dá»‹ch vá»¥ Google; {e}")
    except Exception as e:
        print(f"DEBUG_STT: Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi nháº­n dáº¡ng: {e}")
    speech_thread_active = False
    return recognized_text_output


def doc_van_ban(text_to_speak):
    """Chá»©c nÄƒng chuyá»ƒn vÄƒn báº£n thÃ nh giá»ng nÃ³i."""
    global tts_thread_active
    try:
        engine = pyttsx3.init()
        voices = engine.getProperty('voices')
        vietnamese_voice_id = next(
            (voice.id for voice in voices if "vietnamese" in voice.name.lower() or "an" in voice.name.lower()), None)
        if vietnamese_voice_id:
            engine.setProperty('voice', vietnamese_voice_id)
        else:
            print("DEBUG_TTS: âš ï¸ KhÃ´ng tÃ¬m tháº¥y giá»ng TV, dÃ¹ng giá»ng máº·c Ä‘á»‹nh.")
        engine.say(text_to_speak)
        engine.runAndWait()
    except Exception as e:
        print(f"DEBUG_TTS: Lá»—i khi Ä‘á»c vÄƒn báº£n: {e}")
    finally:
        tts_thread_active = False


def stt_thread_target():
    """HÃ m má»¥c tiÃªu cho luá»“ng nháº­n dáº¡ng giá»ng nÃ³i."""
    global text_display
    recognized_text = nghe_va_chuyen_thanh_van_ban()
    if recognized_text:
        if text_display and not text_display.endswith(" "):
            text_display += " " + recognized_text
        else:
            text_display += recognized_text


def tts_thread_target(text_to_speak):
    """HÃ m má»¥c tiÃªu cho luá»“ng Ä‘á»c vÄƒn báº£n."""
    doc_van_ban(text_to_speak)


def display_mode_selection_screen():
    """Hiá»ƒn thá»‹ mÃ n hÃ¬nh chá»n cháº¿ Ä‘á»™ vÃ  chá» ngÆ°á»i dÃ¹ng nháº­p."""
    screen_width, screen_height = 800, 600
    background = np.zeros((screen_height, screen_width, 3), dtype=np.uint8)
    font = cv2.FONT_HERSHEY_SIMPLEX
    title_text = "CHON CHE DO NHAP LIEU"
    (text_w, text_h), _ = cv2.getTextSize(title_text, font, 1.2, 3)
    cv2.putText(background, title_text, ((screen_width - text_w) // 2, 100), font, 1.2, (0, 255, 0), 3, cv2.LINE_AA)
    options = ["1: NHAN DANG CU CHI TAY", "2: NOI CHUYEN SANG VAN BAN", "3: GO VAN BAN BANG BAN PHIM"]
    for i, option in enumerate(options):
        (text_w, text_h), _ = cv2.getTextSize(option, font, 1, 2)
        cv2.putText(background, option, ((screen_width - text_w) // 2, 250 + i * 50), font, 1, (255, 255, 255), 2,
                    cv2.LINE_AA)
    cv2.putText(background, "Nhan 'Q' de thoat chuong trinh.", (50, screen_height - 50), font, 0.7, (150, 150, 150), 1,
                cv2.LINE_AA)
    cv2.imshow("Chon Che Do", background)
    selected_mode = None
    while selected_mode is None:
        key = cv2.waitKey(1) & 0xFF
        if key == ord('1'):
            selected_mode = 'gesture'
        elif key == ord('2'):
            selected_mode = 'speech'
        elif key == ord('3'):
            selected_mode = 'typing'
        elif key == ord('q'):
            selected_mode = 'quit'
    cv2.destroyWindow("Chon Che Do")
    return selected_mode


def print_instructions():
    """In ra hÆ°á»›ng dáº«n sá»­ dá»¥ng cho ngÆ°á»i dÃ¹ng (phiÃªn báº£n cuá»‘i cÃ¹ng)."""
    print("\n" + "=" * 50)
    print("--- HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG CHUNG ---")
    print("  'q'      : ThoÃ¡t toÃ n bá»™ chÆ°Æ¡ng trÃ¬nh")
    print("  'm'      : Quay láº¡i mÃ n hÃ¬nh chá»n cháº¿ Ä‘á»™ (Menu)")
    print("  'f'      : Báº­t/Táº¯t cháº¿ Ä‘á»™ ToÃ n mÃ n hÃ¬nh (Fullscreen)")
    print("  'r'      : XÃ³a (Reset) toÃ n bá»™ vÄƒn báº£n hiá»‡n táº¡i")
    print("  'b'      : XÃ³a lÃ¹i (Backspace) kÃ½ tá»± cuá»‘i cÃ¹ng")
    print("  't'      : Äá»c (Text-to-speech) vÄƒn báº£n hiá»‡n táº¡i")
    print("  SPACEBAR : ThÃªm dáº¥u cÃ¡ch")
    print("\n--- TÃ™Y CHá»ŒN THEO CHáº¾ Äá»˜ ---")
    print("  CHáº¾ Äá»˜ Cá»¬ CHá»ˆ: Giá»¯ kÃ½ hiá»‡u á»•n Ä‘á»‹nh 2 giÃ¢y Ä‘á»ƒ xÃ¡c nháº­n.")
    print("  CHáº¾ Äá»˜ NÃ“I    : Nháº¥n 'l' Ä‘á»ƒ báº¯t Ä‘áº§u nghe (Listen).")
    print("  CHáº¾ Äá»˜ GÃ•     : GÃµ phÃ­m bÃ¬nh thÆ°á»ng.")
    print("=" * 50 + "\n")


def main_app():
    """HÃ m chÃ­nh cháº¡y toÃ n bá»™ á»©ng dá»¥ng."""
    global text_display, speech_thread_active, tts_thread_active, current_mode

    is_fullscreen = False
    WINDOW_NAME = "Giao Dien Dieu Khien"

    current_mode = display_mode_selection_screen()
    if current_mode == 'quit': return

    text_display = ""
    candidate_char = ""
    candidate_char_start_time = 0
    CONFIRMATION_DURATION = 2.0
    last_char_added_to_text_display = ""
    last_time_char_added_to_text_display = 0
    MIN_INTERVAL_FOR_REPEATED_CHAR_IN_TEXT = 1.5

    # HÃ ng Ä‘á»£i Ä‘á»ƒ lÃ m mÆ°á»£t dá»± Ä‘oÃ¡n
    prediction_history = deque(maxlen=5)

    model = None
    if current_mode == 'gesture':
        try:
            model = joblib.load("sign_model.pkl")
            print("DEBUG_MAIN: âœ… Model 'sign_model.pkl' Ä‘Ã£ Ä‘Æ°á»£c táº£i.")
        except Exception as e:
            print(f"DEBUG_MAIN: âŒ Lá»—i khi táº£i model: {e}");
            return

    mp_hands = mp.solutions.hands
    mp_drawing = mp.solutions.drawing_utils
    cap = cv2.VideoCapture(0)
    if not cap.isOpened(): print("DEBUG_MAIN: âŒ Lá»–I: KhÃ´ng thá»ƒ má»Ÿ webcam."); return

    mic_icon, tts_icon = None, None
    try:
        ICON_SIZE = (50, 50)
        if os.path.exists("mic_on.png"): mic_icon = cv2.resize(cv2.imread("mic_on.png", cv2.IMREAD_UNCHANGED),
                                                               ICON_SIZE)
        if os.path.exists("tts_on.png"): tts_icon = cv2.resize(cv2.imread("tts_on.png", cv2.IMREAD_UNCHANGED),
                                                               ICON_SIZE)
    except Exception as e:
        print(f"DEBUG_MAIN: âš ï¸ Lá»–I: KhÃ´ng táº£i Ä‘Æ°á»£c icon. Lá»—i: {e}")

    cv2.namedWindow(WINDOW_NAME, cv2.WINDOW_NORMAL)
    print_instructions()

    with mp_hands.Hands(
            max_num_hands=1,
            min_detection_confidence=0.7,
            min_tracking_confidence=0.5
    ) as hands:
        while True:
            ret, frame = cap.read()
            if not ret: break

            frame = cv2.flip(frame, 1)
            frame_height, frame_width, _ = frame.shape
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = hands.process(rgb_frame)

            stable_prediction = ""

            if current_mode == 'gesture' and model and results.multi_hand_landmarks:
                hand_landmarks = results.multi_hand_landmarks[0]
                mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

                landmarks_data = []
                base_x, base_y, base_z = hand_landmarks.landmark[0].x, hand_landmarks.landmark[0].y, \
                hand_landmarks.landmark[0].z
                for lm in hand_landmarks.landmark:
                    landmarks_data.extend([lm.x - base_x, lm.y - base_y, lm.z - base_z])

                raw_prediction = model.predict([landmarks_data])[0]
                prediction_history.append(raw_prediction)

                # --- ÄOáº N CODE ÄÃƒ ÄÆ¯á»¢C Sá»¬A Lá»–I ---
                if len(prediction_history) == prediction_history.maxlen:
                    try:
                        stable_prediction = statistics.mode(prediction_history)
                    except statistics.StatisticsError:
                        # Náº¿u khÃ´ng cÃ³ giÃ¡ trá»‹ mode duy nháº¥t (vÃ­ dá»¥: ['A','A','B','B']),
                        # láº¥y giÃ¡ trá»‹ cuá»‘i cÃ¹ng lÃ m dá»± Ä‘oÃ¡n táº¡m thá»i.
                        stable_prediction = prediction_history[-1]
                # ----------------------------------

                if stable_prediction:
                    if stable_prediction == candidate_char:
                        if (time.time() - candidate_char_start_time) >= CONFIRMATION_DURATION:
                            if stable_prediction != last_char_added_to_text_display or (
                                    time.time() - last_time_char_added_to_text_display > MIN_INTERVAL_FOR_REPEATED_CHAR_IN_TEXT):
                                text_display += stable_prediction
                                last_char_added_to_text_display = stable_prediction
                                last_time_char_added_to_text_display = time.time()
                                prediction_history.clear()
                    else:
                        candidate_char = stable_prediction
                        candidate_char_start_time = time.time()
            else:
                prediction_history.clear()
                candidate_char = ""

            # --- Giao diá»‡n Overlay ---
            overlay_height = 120;
            cv2.rectangle(frame, (5, 10), (frame_width - 5, overlay_height), (255, 255, 255), -1)
            mode_status_text = f"MODE: {current_mode.upper()}"
            if current_mode == 'gesture': mode_status_text += f" | Dang cho: {candidate_char}" if candidate_char else ""
            cv2.putText(frame, mode_status_text, (15, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (0, 0, 200), 2)
            display_text_on_frame = "..." + text_display[-(int((frame_width - 30) / 12) - 3):] if len(
                text_display) > int((frame_width - 30) / 12) else text_display
            cv2.putText(frame, f"Van ban: {display_text_on_frame}", (15, 100), cv2.FONT_HERSHEY_COMPLEX, 0.7, (0, 0, 0),
                        2)

            def draw_icon(target_frame, icon, x, y):
                try:
                    h, w, channels = icon.shape
                    if channels < 4: return
                    alpha = icon[:, :, 3] / 255.0
                    roi = target_frame[y:y + h, x:x + w]
                    for c in range(3): roi[:, :, c] = (alpha * icon[:, :, c] + (1.0 - alpha) * roi[:, :, c])
                except Exception:
                    pass

            if speech_thread_active and mic_icon is not None: draw_icon(frame, mic_icon,
                                                                        frame_width - ICON_SIZE[0] - 15, 15)
            if tts_thread_active and tts_icon is not None: draw_icon(frame, tts_icon, frame_width - ICON_SIZE[0] - 15,
                                                                     15)

            cv2.imshow(WINDOW_NAME, frame)
            key = cv2.waitKey(1) & 0xFF

            # --- Khá»‘i xá»­ lÃ½ phÃ­m báº¥m ---
            if key == ord('q'):
                break
            elif key == ord('m'):
                current_mode = display_mode_selection_screen()
                if current_mode == 'quit': break
                text_display = "";
                model = None;
                prediction_history.clear();
                candidate_char = ""
                if current_mode == 'gesture':
                    try:
                        model = joblib.load("sign_model.pkl")
                    except Exception as e:
                        print(f"Lá»—i táº£i láº¡i model: {e}")
                print_instructions();
                continue
            elif key == ord('f'):
                is_fullscreen = not is_fullscreen
                cv2.setWindowProperty(WINDOW_NAME, cv2.WND_PROP_FULLSCREEN,
                                      cv2.WINDOW_FULLSCREEN if is_fullscreen else cv2.WINDOW_NORMAL)
            elif key == ord('r'):
                text_display = ""
            elif key == ord('b'):
                text_display = text_display[:-1]
            elif key == ord(' '):
                text_display += " "
            elif key == ord('t'):
                if not tts_thread_active and text_display:
                    tts_thread_active = True;
                    threading.Thread(target=tts_thread_target, args=(text_display,)).start()

            if current_mode == 'speech' and key == ord('l'):
                if not speech_thread_active:
                    speech_thread_active = True;
                    threading.Thread(target=stt_thread_target).start()
            elif current_mode == 'typing':
                if 32 <= key <= 126: text_display += chr(key)

    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    main_app()